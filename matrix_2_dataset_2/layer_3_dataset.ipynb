{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoch = 20\n",
    "# tensorboard_writer = SummaryWriter()\n",
    "T_0 =3\n",
    "T_mult=2\n",
    "eta_min = 1e-6\n",
    "epsilon = 0.01\n",
    "inv_base_matrix = [[[ 1.5,-1],[-1, 1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4,32)\n",
    "        self.fc2 = nn.Linear(32,32)\n",
    "        self.fc3 = nn.Linear(32, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)   # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = BaseModel()\n",
    "model = model.to(torch.double)\n",
    "model = model.to('cuda') \n",
    "model.train()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.dataset = np.load(root_dir)\n",
    "        print('number of data points', self.dataset.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        x = self.dataset[idx, :,:,0]\n",
    "        y = self.dataset[idx, :,:,1]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points 1000000\n",
      "number of data points 10000\n"
     ]
    }
   ],
   "source": [
    "train_set = CustomDataset('train_set.npy')\n",
    "val_set = CustomDataset('val_set.npy')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-7)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer,T_0,T_mult,eta_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\D_software\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:971: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 7800, Loss: 0.0001, lr: 0.00005000\n",
      "\n",
      "Train Step: 15600, Loss: 0.0000, lr: 0.00003775\n",
      "\n",
      "Train Step: 23400, Loss: 0.0000, lr: 0.00001325\n",
      "\n",
      "Train Step: 31200, Loss: 0.0000, lr: 0.00005000\n",
      "\n",
      "Train Step: 39000, Loss: 0.0000, lr: 0.00004672\n",
      "\n",
      "Train Step: 46800, Loss: 0.0000, lr: 0.00003775\n",
      "\n",
      "Train Step: 54600, Loss: 0.0000, lr: 0.00002550\n",
      "\n",
      "Train Step: 62500, Loss: 0.0000, lr: 0.00001325\n",
      "\n",
      "Train Step: 70300, Loss: 0.0000, lr: 0.00000428\n",
      "\n",
      "Train Step: 78100, Loss: 0.0000, lr: 0.00005000\n",
      "\n",
      "Train Step: 85900, Loss: 0.0000, lr: 0.00004917\n",
      "\n",
      "Train Step: 93700, Loss: 0.0000, lr: 0.00004672\n",
      "\n",
      "Train Step: 101500, Loss: 0.0000, lr: 0.00004282\n",
      "\n",
      "Train Step: 109300, Loss: 0.0000, lr: 0.00003775\n",
      "\n",
      "Train Step: 117100, Loss: 0.0000, lr: 0.00003184\n",
      "\n",
      "Train Step: 125000, Loss: 0.0000, lr: 0.00002550\n",
      "\n",
      "Train Step: 132800, Loss: 0.0000, lr: 0.00001916\n",
      "\n",
      "Train Step: 140600, Loss: 0.0000, lr: 0.00001325\n",
      "\n",
      "Train Step: 148400, Loss: 0.0000, lr: 0.00000818\n",
      "\n",
      "Train Step: 156200, Loss: 0.0000, lr: 0.00000428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_accu = []\n",
    "i = 1\n",
    "for epoch in range(epoch):\n",
    "    for data, target in train_loader:\n",
    "#         target = target - torch.tensor(inv_base_matrix)\n",
    "#         print(target)\n",
    "#         target = target/epsilon\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target.view(-1,4))\n",
    "        loss.backward()\n",
    "        \n",
    "        mse_loss = loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if i % 10 == 0:\n",
    "#             tensorboard_writer.add_scalar(\"Loss/step\", loss, i)\n",
    "        if i % 100 == 0:\n",
    "            print('\\rTrain Step: %d, Loss: %.4f, lr: %.8f'%(i, mse_loss, scheduler.get_lr()[0]), end=\"\")\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'layer3_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 530.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.463188810197244e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_accu = 0\n",
    "i = 1\n",
    "model = model.eval()\n",
    "total_error = 0.0\n",
    "total_number = 0\n",
    "for data, target in tqdm(val_loader):\n",
    "    target = target - torch.tensor(inv_base_matrix)\n",
    "#     target = target/epsilon\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data = data.to('cuda')\n",
    "#     target = target.to('cuda')\n",
    "    output = model(data)\n",
    "    output = output.detach().to('cpu') - torch.tensor(inv_base_matrix).view(-1,4)\n",
    "#     output = output/epsilon\n",
    "    total_error += torch.sum(torch.abs(output[:,:] - target.view(-1,4)[:,:]))\n",
    "    total_number += output.shape[0]*output.shape[1]\n",
    "#     print(total_error/total_number)\n",
    "#     print(output[:5,:])\n",
    "#     print(target.view(-1,4)[:5,:])\n",
    "#     break\n",
    "\n",
    "print(total_error.numpy()/total_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "model 1 test error: 4.8965896880933165e-06\n",
    "model 2 test error: 9.76252439105381e-06\n",
    "model 3 test error: 7.463188810197244e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3741009631147895e-06\n",
      "3.95018841116143e-12\n"
     ]
    }
   ],
   "source": [
    "temp = [ 4.8965896880933165e-06, 9.76252439105381e-06,7.463188810197244e-06]\n",
    "print(np.mean(temp))\n",
    "print(np.var(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
